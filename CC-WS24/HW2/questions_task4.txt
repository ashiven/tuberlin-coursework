1)What happens here?


A Spark master node is started at 10.0.0.77:7077.
Two worker nodes successfully register with the master node:
Worker 1: 10.0.0.80:33546 (2 cores, 2GB RAM).
Worker 2: 10.0.0.100:44916 (2 cores, 2GB RAM).

The JavaSparkPi application registers with the master
The master launches executors on both worker nodes:
Executor 0 is launched on Worker 2 (10.0.0.100).
Executor 1 is launched on Worker 1 (10.0.0.80).
Each executor processes its assigned tasks, computing whether randomly generated points fall within a unit circle.

Both initial executors exit with error code 1
Spark detects the failure and automatically launches a new executor to retry the task
Executor 2 replaces Executor 0 on worker 10.0.0.100
Executor 3 replaces Executor 1 on worker 10.0.0.80
The tasks are retried on the new executors, ensuring successful completion. The master aggregates results from all tasks and computes the approximate value of π. Application completes successfully, and resources are cleaned up by both the master and worker nodes.



2)Program structured and tasks distributed


The program starts by creating a SparkSession, the entry point to the Spark application.
A JavaSparkContext is derived from the session to manage RDDs (Resilient Distributed Datasets) and interact with the cluster.

Input parameter: 500 slices (partitions)
Total points: n = 100000 * 500 = 50 million points
The computation is distributed across 2 worker nodes, each with 2 cores

Creates a SparkSession and JavaSparkContext
Generates a list of n integers (0 to n-1)
Splits the data into specified number of slices (500)

For each number in the dataset:
Generates random points (x,y) in a 2x2 square (-1 to 1)
Checks if point falls within unit circle (x² + y² ≤ 1)
Returns 1 if inside, 0 if outside

Reduces results by summing all points inside circle
Calculates π ≈ 4 * (points_inside_circle / total_points)
SparkSession stopped, all resources are released.


Spark initializes a distributed computing environment where the master node coordinates the tasks. 
The data is divided into multiple segments and distributed across the cluster for parallel processing. 
The dataset is divided into 500 partitions, each of which is assigned to a different core on the Worker node. 
Worker 1 and Worker 2 each have 2 cores, enabling 4 tasks to be processed simultaneously. 
Each partition is processed independently by the map function.
When initial tasks fail , Spark automatically retries them on newly launched executors. 
Tasks are scheduled by the Master and executed by the Workers. 
Reduce operation triggers the execution, merging the results of all the partitions.